{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "house_df = pd.read_csv('kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# something is bugged\n",
    "# this can't be together with imports: https://github.com/jupyter/notebook/issues/3385\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300.0 # default: 72.0\n",
    "mpl.rcParams['figure.figsize'] = [8., 4.5] # default: [6., 4.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression. A bit of formalism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a data consisting of $n$ samples. These samples are $(m+1)$ - dimensional vectors, where first $m$ dimensions are called features (explanatory variables) and will be used to predict the last dimension which is called regressand (dependent variable). \n",
    "\n",
    "We will have therefore a $n \\times m$ matrix **$X$** (called feature matrix) and vector **$y$** of lenght $n$. \n",
    "\n",
    "In our simple example, living squarefeet is a feature, house's price is regressand and the data consists of $100$ samples. It forms $100 \\times 1$ feature matrix **$X$** and vector **$y$** of length $100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = house_df.sqft_living.values.reshape(-1,1)[:100]\n",
    "y = house_df.price.values.reshape(-1,1)[:100]\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression approach tries to find a vector **$b$** which minimizes the cost function \n",
    "\n",
    "$$f_{y}(b) = \\dfrac{|Xb - y|^2}{n}, $$\n",
    "\n",
    "where **$|\\cdot|$** is euclidean norm and **$Xb$** is simple matrix multiplication (vector is also a matrix). \n",
    "\n",
    "Geometrically (and roughly) speaking, we are determining a line which minimizes the cumulative distance to all the points. \n",
    "\n",
    "When such a vector **$b$** is found, we can predict values **$y$** for given features **$X$** by calculating **$Xb$**. We have therefore \n",
    "\n",
    "$$y_{pred} = Xb, \\\\ f_{y}(b) = \\dfrac{|y_{pred} - y|^2}{n}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "a) Create functions *predict(b, X)* and *cost(y_pred, y_true)* implementing theory above.  All inputs should be numpy arrays (take care of dimension scalability!)  \n",
    "b) For our data **$X,y$**, plot cost as the function of **$b$** (which is one dimensional vector). Plot the line with best slope among  evaluated with *cost* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a)\n",
    "def predict(b, X):\n",
    "    return np.dot(X, b)\n",
    "\n",
    "def cost(y_pred, y_true):\n",
    "   #return np.linalg.norm(y_pred - y_true) ** 2 / len(y_pred)\n",
    "    return np.sum((y_pred - y_true) ** 2) / len(y_pred)\n",
    "\n",
    "#short test:\n",
    "test_b = np.array([[1],[1],[1]])\n",
    "test_X = np.array([[1,2,3],[2,5,4],[3,4,5],[4,5,7]])\n",
    "test_y = np.array([[1],[2],[3],[4]])\n",
    "\n",
    "prediction = predict(test_b, test_X)\n",
    "print(prediction)\n",
    "print(cost(prediction, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b)\n",
    "fy = lambda b: cost(predict(b, X), y)\n",
    "\n",
    "t = np.arange(-1000, 1000, 0.25)\n",
    "ft = list(map(fy, t))\n",
    "plt.plot(t, ft)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(t[4800:5200], ft[4800:5200])\n",
    "plt.show()\n",
    "\n",
    "from scipy.optimize import fmin\n",
    "min_fy = fmin(fy, 250, disp=False)[0]\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "min_fy2 = minimize(fy, 200).x[0]\n",
    "\n",
    "min_fy3 = t[np.argmin([fy(b) for b in t])]\n",
    "\n",
    "plt.scatter(X, y, color='red')\n",
    "plt.plot(X, predict(min_fy, X), label=f'bias = {min_fy}, cost = {cost(predict(min_fy, X), y)}')\n",
    "plt.plot(X, predict(min_fy2, X), label=f'bias = {min_fy2}, cost = {cost(predict(min_fy2, X), y)}')\n",
    "plt.plot(X, predict(min_fy3, X), label=f'bias = {min_fy3}, cost = {cost(predict(min_fy3, X), y)}')\n",
    "plt.legend()\n",
    "plt.gca().set_xlim([0,6000])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bias\n",
    "\n",
    "The line above has limitation of crossing point (0,0). As on our data it doesn't seem harmful, let us check how our model would perform, if all $y$ were shifted by some constant. The interpretation is as follows: the goverment has imposed a new tax in the set amount of 1 million dollar on buying new houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, for the best slope the error has grown almost by the factor of four. In addition we clearly see, that what limits us is indeed the constraint of all lines crossing point $(0,0)$.   \n",
    "\n",
    "There are two basic solutions of this problem. One of them is centring of the data, that is substracting overall mean from all the values of given feature (or regressand).  \n",
    "\n",
    "The other is adding bias term, which in this context (different than context discussed during the lectures!) may be understood as constant term in line equation. We simulate adding constant term to result of each prediction by appending artificial feature consisting of ones to **$X$** matrix and additional term to **$b$** vector.\n",
    "\n",
    "## Exercise 2\n",
    "a) Add bias column to the **$X$** matrix.  \n",
    "b) Make the code below compatible with functions that you've created so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a)\n",
    "if X.shape[1] == 1:\n",
    "    X = np.insert(X, 1, 1, axis=1)\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b)\n",
    "y_shifted = y+1e6\n",
    "\n",
    "#No Bias Part\n",
    "b_arr = [np.array(b1) for b1 in np.linspace(-1000, 1000, num=2001)]\n",
    "cost_arr = [cost(predict(b, X[:,0]), y_shifted) for b in b_arr]\n",
    "best_b1 = b_arr[np.argmin(cost_arr)]\n",
    "\n",
    "\n",
    "#Bias Part\n",
    "nb_of_bs = 101 # compute the cost nb_of_bs times in each dimension\n",
    "b1 = np.linspace(-500, 2000, num=nb_of_bs) # slope coefficient\n",
    "b2 = np.linspace(-1e5, 3e6, num=nb_of_bs) # bias\n",
    "b_x, b_y = np.meshgrid(b1, b2) # generate grid\n",
    "cost_arr_2d = np.zeros((nb_of_bs, nb_of_bs)) # initialize cost matrix\n",
    "\n",
    "# Fill the cost matrix for each combination of coefficients\n",
    "for i in range(nb_of_bs):\n",
    "    for j in range(nb_of_bs):\n",
    "        cost_arr_2d[i,j] = cost(predict(np.array([[b_x[i,j]], [b_y[i,j]]]), X) , y_shifted)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,20)) \n",
    "        \n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"Cost heatmap for bias and slope\")\n",
    "\n",
    "plt.contourf(b_x, b_y, np.log(cost_arr_2d), 20, alpha=0.9, cmap=cm.pink)\n",
    "cbar = plt.colorbar()\n",
    "plt.scatter(best_b1, 0, label = \"Best solution without bias\")\n",
    "cbar.ax.set_ylabel('log(cost)')\n",
    "plt.xlabel(\"b1 (slope)\")\n",
    "plt.ylabel(\"b2 (bias)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "plt.scatter(X.T[0], y_shifted)\n",
    "plt.xlabel(\"living sqft\")\n",
    "plt.ylabel(\"price\")\n",
    "\n",
    "x_model = np.linspace(np.min(X), np.max(X), 1000)\n",
    "y_model = best_b1*x_model\n",
    "plt.plot(x_model, y_model, label='No Bias. Best b1 = {}, error = {:.2E}'.format(best_b1, cost_arr[np.argmin(cost_arr)]))\n",
    "\n",
    "best_b1_2d_ind, best_b2_2d_ind = np.unravel_index(cost_arr_2d.argmin(), cost_arr_2d.shape)\n",
    "best_b1 = b_x[best_b1_2d_ind,best_b2_2d_ind]\n",
    "best_b2 = b_y[best_b1_2d_ind,best_b2_2d_ind]\n",
    "\n",
    "y_model = best_b1*x_model + best_b2\n",
    "plt.plot(x_model, y_model, label='Best b1 = {}, b2 = {}, error = {:.2E}'.format(best_b1, best_b2, cost_arr_2d[best_b1_2d_ind,best_b2_2d_ind]))\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding bias improved error by three orders of magnitude. Not bad!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Standardization\n",
    "\n",
    "In order to improve performance of many machine learning algorithms, the value standardization is applied. It makes the values of each feature in the data have zero-mean and unit-variance. It is achieved by substracting from each value $x$ the overall mean $\\bar{x}$, and then dividing it by feature's standard deviation ${\\sigma}$:\n",
    "\n",
    "$$x' = \\frac{x - \\bar{x}}{\\sigma}. $$\n",
    "\n",
    "It is important to remember, that bias column should not be standardize!\n",
    "\n",
    "## Exercise 3\n",
    "a) Implement function standardize(), which standardize the feature matrix and returns it together with two vectors containing original features' means and standard deviations.  \n",
    "b) Implement function destandardize(), which restores the original data given standardized feature matrix X and vectors containing original features' means and standard deviations.  \n",
    "c) Plot original $X,y$ data. Then plot it after standardization. Then plot it after destandardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a)\n",
    "def standardize(X):\n",
    "    X_std = np.std(X, axis=0)\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    return (X - X_mean) / X_std, X_mean, X_std\n",
    "\n",
    "#b)\n",
    "def destandardize(X, means, sds):\n",
    "    return X * sds + means\n",
    "\n",
    "\n",
    "#short test:\n",
    "W = np.array([[1,2,3],[2,5,4],[3,4,5],[4,5,7]])\n",
    "W2, W_mean, W_std = standardize(W)\n",
    "print(W2)\n",
    "print(W_mean)\n",
    "print(W_std)\n",
    "print(destandardize(W2, W_mean, W_std))\n",
    "\n",
    "#Hint: with np.mean and np.std first function is three lines long. The second is one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c)\n",
    "\n",
    "if X.shape[1] == 2:\n",
    "    X = np.delete(X, 1, axis=1)\n",
    "\n",
    "X2, X_mean, X_std = standardize(X)\n",
    "y2, y_mean, y_std = standardize(y)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(X2, y2)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(destandardize(X2, X_mean, X_std), destandardize(y2, y_mean, y_std))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous examples we were determining the approximate value of optimal vector **$b$** by finding best arguments from pre-defined grid. This solution is neither fast nor precise. Although in theory it is possible to find vector **$b$** analytically, it requires inversing large matrices, as the close form for **$b$** is $b=(X^\\mathsf{T}X)^{-1}X^\\mathsf{T}y$.\n",
    "\n",
    "The other approach is to find approximation of **$b$** is Gradient Descent Method. Let us recall that for function $f: \\ R^n \\to R$ a gradient \n",
    "\n",
    "$$\\nabla f =  \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$$\n",
    "\n",
    "is the direction of the biggest increase of $f$. Using linearity of derivative, $-\\nabla f$ is the direction of the biggest decrease. Gradient Descent Method is based on iterative moving in the direction opposite to gradient, and by these means approaching the minimum.\n",
    "\n",
    "#### GDM step by step:  \n",
    "1. Choose starting point $x_{0}$ and parameters: *learning rate* and *precision*,\n",
    "2. Update $x_{i+1} = x_{i} - \\nabla f(x_{i})\\cdot lr$, where $lr$ is learning rate parameter,\n",
    "3. If $(|\\nabla f(x_{i})| < precision)$, end. If not, go back to point 2.\n",
    "\n",
    "One of many limitations of GMD may be knowledge of function's gradient. Luckily for us, it is quite easy in case of linear regression with mean square error cost function. We have\n",
    "\n",
    "$$ \\nabla f_{y}(b) = \\nabla\\dfrac{|Xb−y|^2}{n} = \\dfrac{2X^\\mathsf{T}(Xb−y)}{n}.  $$\n",
    "\n",
    "## Exercise 4\n",
    "a) Create functions: *gradient()* which computes gradient for linear regresion and *gradient_descent_step()* which returns new vector **b** being the result of one GDM step.  \n",
    "\n",
    "b) Take **$X$** consisting of *sqft_living* together with bias column. Set **$y$** as price. Standardize both **$X$** (without bias) and **$y$**. Plot a heatmap showing dependence of cost function's value on vector **$b$**. Mark first 5 steps of gradient descent with *learning rate* = $0.2$, starting from **$b =  \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}$**. Experiment with other *learning rates*.  \n",
    "\n",
    "c) Take **$X$** consisting of *sqft_living* and *sqft_lot*. Set **$y$** as price. Standardize both **$X$** and **$y$**. Plot a heatmap showing dependence of cost function's value on vector **$b$**. Mark first 5 steps of gradient descent with *learning rate* = $0.2$, starting from **$b =  \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}$**. Experiment with other *learning rates*.  \n",
    "\n",
    "d) Try to redo above points without standarization. How can you explain such results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a)\n",
    "\n",
    "def gradient(b, X, y):\n",
    "    return 2 * X.T.dot(X.dot(b) - y) / len(X)\n",
    "\n",
    "def gradient_descent_step(b, X, y, lr):\n",
    "    return b - gradient(b, X, y).dot(lr)\n",
    "\n",
    "\n",
    "#short test:\n",
    "test_b = np.array([[1],[1],[1]])\n",
    "test_X = np.array([[1,2,3],[2,5,4],[3,4,5],[4,5,7]])\n",
    "test_y = np.array([[1],[2],[3],[4]])\n",
    "\n",
    "print(gradient(test_b, test_X, test_y))\n",
    "print(gradient_descent_step(test_b, test_X, test_y, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b)\n",
    "X = house_df.sqft_living.values.reshape(-1,1)[:]\n",
    "y = house_df.price.values.reshape(-1,1)[:]\n",
    "\n",
    "X2, X_mean, X_std = standardize(X)\n",
    "y2, y_mean, y_std = standardize(y)\n",
    "\n",
    "X2 = np.insert(X2, 1, 1, axis=1)\n",
    "\n",
    "# Plot a heatmap showing dependence of cost function's value on vector b.\n",
    "nb_of_bs = 101 # compute the cost nb_of_bs times in each dimension\n",
    "b1 = np.linspace(-4, 4, num=nb_of_bs) # slope coefficient\n",
    "b2 = np.linspace(-4, 4, num=nb_of_bs) # bias\n",
    "b_x, b_y = np.meshgrid(b1, b2) # generate grid\n",
    "cost_arr_2d = np.zeros((nb_of_bs, nb_of_bs)) # initialize cost matrix\n",
    "\n",
    "# Fill the cost matrix for each combination of coefficients\n",
    "for i in range(nb_of_bs):\n",
    "    for j in range(nb_of_bs):\n",
    "        cost_arr_2d[i,j] = cost(\n",
    "            predict(\n",
    "                np.array(\n",
    "                    [\n",
    "                        [b_x[i, j]],\n",
    "                        [b_y[i, j]]\n",
    "                    ]\n",
    "                ),\n",
    "                X2\n",
    "            ),\n",
    "            y2\n",
    "        )\n",
    "\n",
    "plt.title(\"Cost heatmap for bias and slope\")\n",
    "plt.contourf(\n",
    "    b_x,\n",
    "    b_y,\n",
    "    np.log(cost_arr_2d),\n",
    "    20,\n",
    "    #alpha=0.9,\n",
    "    cmap=cm.pink,\n",
    ")\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('log(cost)')\n",
    "plt.xlabel(\"b1 (slope)\")\n",
    "plt.ylabel(\"b2 (bias)\")\n",
    "\n",
    "b = [np.array([[3], [2]])] # np.array jest niepotrzebne, ale powinno szybciej działać\n",
    "rate = 0.2\n",
    "\n",
    "for i in range(5):\n",
    "    b.append(gradient_descent_step(b[-1], X2, y2, rate))\n",
    "plt.plot(*zip(*b), marker='.') # *zip, bo potrzebujemy 2 współrzędne rozbić na 2 tablice\n",
    "\n",
    "# etykiety\n",
    "for i in range(len(b)):\n",
    "    plt.text(b[i][0] - 0.05, b[i][1] - 0.2, f'w({i})', color='blue', fontsize=7, rotation=-55)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Mark first 5 steps of gradient descent with learning rate = 0.2, starting from b=[32].\n",
    "# Experiment with other learning rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c)\n",
    "X = house_df[['sqft_living', 'sqft_lot']].values.reshape(-1,2)[:]\n",
    "y = house_df.price.values.reshape(-1,1)[:]\n",
    "\n",
    "X2, X_mean, X_std = standardize(X)\n",
    "y2, y_mean, y_std = standardize(y)\n",
    "\n",
    "# Plot a heatmap showing dependence of cost function's value on vector b.\n",
    "nb_of_bs = 101 # compute the cost nb_of_bs times in each dimension\n",
    "b1 = np.linspace(-5, 5, num=nb_of_bs) # slope coefficient\n",
    "b2 = np.linspace(-4, 4, num=nb_of_bs) # bias\n",
    "b_x, b_y = np.meshgrid(b1, b2) # generate grid\n",
    "cost_arr_2d = np.zeros((nb_of_bs, nb_of_bs)) # initialize cost matrix\n",
    "\n",
    "# Fill the cost matrix for each combination of coefficients\n",
    "for i in range(nb_of_bs):\n",
    "    for j in range(nb_of_bs):\n",
    "        cost_arr_2d[i,j] = cost(\n",
    "            predict(\n",
    "                np.array(\n",
    "                    [\n",
    "                        [b_x[i, j]],\n",
    "                        [b_y[i, j]]\n",
    "                    ]\n",
    "                ),\n",
    "                X2\n",
    "            ),\n",
    "            y2\n",
    "        )\n",
    "\n",
    "plt.contourf(\n",
    "    b_x,\n",
    "    b_y,\n",
    "    np.log(cost_arr_2d),\n",
    "    20,\n",
    "    #alpha=0.9,\n",
    "    cmap=cm.pink,\n",
    ")\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('log(cost)')\n",
    "plt.xlabel(\"sqft_living\")\n",
    "plt.ylabel(\"sqft_lot\")\n",
    "\n",
    "b = [np.array([[3], [1]])] # np.array jest niepotrzebne, ale powinno szybciej działać\n",
    "rate = 0.2\n",
    "\n",
    "for i in range(5):\n",
    "    b.append(gradient_descent_step(b[-1], X2, y2, rate))\n",
    "plt.plot(*zip(*b), marker='.') # *zip, bo potrzebujemy 2 współrzędne rozbić na 2 tablice\n",
    "\n",
    "# etykiety\n",
    "for i in range(len(b)):\n",
    "    plt.text(b[i][0] - 0.05, b[i][1] - 0.2, f'w({i})', color='blue', fontsize=7, rotation=-55)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d)\n",
    "X = house_df.sqft_living.values.reshape(-1,1)[:]\n",
    "y = house_df.price.values.reshape(-1,1)[:]\n",
    "\n",
    "X2 = X # X2, X_mean, X_std = standardize(X)\n",
    "y2 = y # y2, y_mean, y_std = standardize(y)\n",
    "\n",
    "X2 = np.insert(X2, 1, 1, axis=1)\n",
    "\n",
    "# Plot a heatmap showing dependence of cost function's value on vector b.\n",
    "nb_of_bs = 101 # compute the cost nb_of_bs times in each dimension\n",
    "b1 = np.linspace(-4, 4, num=nb_of_bs) # slope coefficient\n",
    "b2 = np.linspace(-4, 4, num=nb_of_bs) # bias\n",
    "b_x, b_y = np.meshgrid(b1, b2) # generate grid\n",
    "cost_arr_2d = np.zeros((nb_of_bs, nb_of_bs)) # initialize cost matrix\n",
    "\n",
    "# Fill the cost matrix for each combination of coefficients\n",
    "for i in range(nb_of_bs):\n",
    "    for j in range(nb_of_bs):\n",
    "        cost_arr_2d[i,j] = cost(\n",
    "            predict(\n",
    "                np.array(\n",
    "                    [\n",
    "                        [b_x[i, j]],\n",
    "                        [b_y[i, j]]\n",
    "                    ]\n",
    "                ),\n",
    "                X2\n",
    "            ),\n",
    "            y2\n",
    "        )\n",
    "\n",
    "plt.title(\"Cost heatmap for bias and slope\")\n",
    "plt.contourf(\n",
    "    b_x,\n",
    "    b_y,\n",
    "    np.log(cost_arr_2d),\n",
    "    20,\n",
    "    #alpha=0.9,\n",
    "    cmap=cm.pink,\n",
    ")\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('log(cost)')\n",
    "plt.xlabel(\"b1 (slope)\")\n",
    "plt.ylabel(\"b2 (bias)\")\n",
    "\n",
    "b = [np.array([[3], [2]])] # np.array jest niepotrzebne, ale powinno szybciej działać\n",
    "rate = 0.2\n",
    "\n",
    "for i in range(5):\n",
    "    b.append(gradient_descent_step(b[-1], X2, y2, rate))\n",
    "plt.plot(*zip(*b), marker='.') # *zip, bo potrzebujemy 2 współrzędne rozbić na 2 tablice\n",
    "\n",
    "# etykiety\n",
    "for i in range(len(b)):\n",
    "    plt.text(b[i][0] - 0.05, b[i][1] - 0.2, f'w({i})', color='blue', fontsize=7, rotation=-55)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = house_df[['sqft_living', 'sqft_lot']].values.reshape(-1,2)[:]\n",
    "y = house_df.price.values.reshape(-1,1)[:]\n",
    "\n",
    "X2 = X # X2, X_mean, X_std = standardize(X)\n",
    "y2 = y # y2, y_mean, y_std = standardize(y)\n",
    "\n",
    "# Plot a heatmap showing dependence of cost function's value on vector b.\n",
    "nb_of_bs = 101 # compute the cost nb_of_bs times in each dimension\n",
    "b1 = np.linspace(-4, 4, num=nb_of_bs) # slope coefficient\n",
    "b2 = np.linspace(-4, 4, num=nb_of_bs) # bias\n",
    "b_x, b_y = np.meshgrid(b1, b2) # generate grid\n",
    "cost_arr_2d = np.zeros((nb_of_bs, nb_of_bs)) # initialize cost matrix\n",
    "\n",
    "# Fill the cost matrix for each combination of coefficients\n",
    "for i in range(nb_of_bs):\n",
    "    for j in range(nb_of_bs):\n",
    "        cost_arr_2d[i,j] = cost(\n",
    "            predict(\n",
    "                np.array(\n",
    "                    [\n",
    "                        [b_x[i, j]],\n",
    "                        [b_y[i, j]]\n",
    "                    ]\n",
    "                ),\n",
    "                X2\n",
    "            ),\n",
    "            y2\n",
    "        )\n",
    "\n",
    "plt.title(\"Cost heatmap for bias and slope\")\n",
    "plt.contourf(\n",
    "    b_x,\n",
    "    b_y,\n",
    "    np.log(cost_arr_2d),\n",
    "    20,\n",
    "    #alpha=0.9,\n",
    "    cmap=cm.pink,\n",
    ")\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('log(cost)')\n",
    "plt.xlabel(\"sqft_living\")\n",
    "plt.ylabel(\"sqft_lot\")\n",
    "\n",
    "b = [np.array([[3], [1]])] # np.array jest niepotrzebne, ale powinno szybciej działać\n",
    "rate = 0.2\n",
    "\n",
    "for i in range(5):\n",
    "    b.append(gradient_descent_step(b[-1], X2, y2, rate))\n",
    "plt.plot(*zip(*b), marker='.') # *zip, bo potrzebujemy 2 współrzędne rozbić na 2 tablice\n",
    "\n",
    "# etykiety\n",
    "for i in range(len(b)):\n",
    "    plt.text(b[i][0] - 0.05, b[i][1] - 0.2, f'w({i})', color='blue', fontsize=7, rotation=-55)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
